\xhdr{4. Extracting Complex Biological Events with Rich Graph-Based Feature Sets - \citeauthor{bjorne}}\\
This paper describes a system for extracting events among genes and protiens from biomedical literature. Their model handles the situations in which an event is an argument to other events, resulting in a nested structure. They divide the task of event extraction into three independent steps considered as machine learning problem making use of features using dependency parse graph - trigger recognition, argument detection and semantic post processing. They claim that by separating trigger recognition from argument detection, they can use methods like named entity recognition to tag words as entities. But, they assume that all entities are recognizable as named entities and are availalbe already. In their model, the sentence or paragraph from which events are to be extracted is represented as a graph with nodes corresponding to entities (or arguments) and events, and the edges corresponding to event arguments. This is a natural representation as it is easy to encode event-event relations as well. As the first step, they tackle the problem of trigger detection as a token labeling problem in which each token is assignned to an event class if it is likely to be an event otherwise, to the negative class. Multi-class SVM is used for the classification task. Token features (related to properties of individual tokens like capitalization, presence of punctuation etc), frequency features (for e.g., number of entities in the sentence), and dependency chain features (encoding dependencies between tokens) of length upto three are used. In the next stage, they have edge detection, modeled again as a multi-class SVM to tag each potential edge between an event and an entity; or an event and an event representing a relation between them. The edge is tagged as {\em theme, cause} or a negative denoting the absense of an edge. The features they use include N-grams (generated by merging attributes of 2-4 consecutive tokens), individual component features, semantic node features (obtained from just the two nodes) and frequency features. In this case, the edges between different nodes (entities and events) are predicted independent of each other. Because of this, they need a third stage in their pipleline to do semantic post processing to ensure that the semantic graph produced by the trigger and edge detection steps does not have any improper combinations. This is a rule based step which refines the graph based on event-event and event-argument types. They also do steps like node duplication in case there are two separate events that are denoted by a single word.

\xhdr{5. Joint Learning Improves Semantic Role Labeling - \citeauthor{toutanova}}
They use a joint model to improve semantic role labeling. In their support, they claim that there are tight dependencies between the different entities and their arguments, because of which, building independent classifiers to handle the task of event extraction assumes a lot of independence which does not really exist in natural language. For instance, there may be hard constraints according to which arguments to events (or predicates) cannot overlap with each other, and also soft constraints by which a predicate cannot have two or more agents as arguments. In this paper, they present a joint model for semantic role labeling using global features to achieve better performance as compared to the state of the art models. In their model, they had two sets of models - local models that learn to role label nodes in the parse tree independently, called as \emph{local models} and models that incorporate dependencies among the labels of multiple nodes, called \emph{joint models}. The local classifier handles the task of semantic role labeling as two separate tasks of identification and classification. The features that they use are tightly bound to the parse tree and parts of speech tags. In the identification phase, each node of the parse tree created from the sentence is tagged as an argument or not, and, in the next stage, each argument is associated with its appropriate semantic role. This is possible because these tasks of identification and classification decomposes well, so that they can be solved separately. Since the local models don't capture the dependencies amonst arguments, they also have a joint model which builds on top of assignments generated by the local model. Their model also does re-ranking of the assignments generated by the local model based on weights learnt from a log-linear re-ranking model. For this, they map the features from a parse tree and the label sequence to a vector space and then maximize the log likelihodd of the best assignments. One important factor that helps their model perform well is the usage of templates to generate features. A template helps capture dependencies between label of a node and input features of other argument nodes - for instance, templates could be used to avoid picking multiple arguments to fill the same semantic role (agent of the verb for instance) or, to count the number of arguments ro the left and right of the predicate. This model gave them better accuracies than the best model. In short, jointly modeling the arguments of verbs does help.

\xhdr{6. Fast and Robust Joint Models for Biomedical Event Extraction - \citeauthor{riedelmc}} introduce three joing models of increasing complexity designed to extract bio-medical events. They formulate the search for event structures as an optimizaton problem through a set of binary variables by projecting events to a graph structure over tokens. For a sentence and a set of candidate trigger token, they label each candidate with an event type, t it is trigger for, or 'None' if it is not a trigger. Hence, for a candidate trigger, there are as many binary variables as the nubmer of possible event types + 1. Let these variables be denoted by ${e}$. For each candidate trigger, they introduce binary variables ${a}$ to asscociate the trigger to all arguments a of the event to which the trigger belongs to. These arguments can be events or entities. This variable are used to add association between event-event and event-entity pairs. This representation has the shortcoming that it is not possible to differentiate between two events with the same trigger but different arguments, or, one event with several argument. While earlier work overcame this by adding adhoc rules, \citeauthor{riedelmc} augmented the graph representation by adding edges between the pair of arguments that are a part of the same event by introducing another binary variable ${b}$. As mentioned before they have three progressive models for event extraction. The first model does joint trigger and argument extraction (by learning the assignment variables ${e}$ and ${a}$) by an efficient exact inference algorithm by independently scoring trigger labels and argument roles and maximizing the sum of the scores both by using a scoring function. Model 1 can predict structures that cannot be mapped to events. For instance, it can label a token that is actually an event as an argument to another event, but, the former event may not be an active trigger or argument. This would not be ideal as we would want events to combine only with events or arguments. Model 2 enforces these constraints by using a scoring function to identify consistent trigger labels and incoming edges. Model 3 predicts the variables that denote argument-argument combinations as described earlier by using a scorign function to learn weights for the binary variable ${b}$. In short, their work model the full process of event extraction as an optimization problem over a set of binary variables.

\xhdr{7. Event Extraction as Dependency Parsing - \citeauthor{mcclosky}} introduce the task of event extraction from text by means of dependency parsing. Nested event structures are common occurrences, but most model before them were incapable of handling them as events and arguments were extracted independently. In this paper, they propose extracting events (including nested events) by taking the tree of event-argument relations and using it directly as a representation in a reranking dependency parser. The entities in this task were a part of the dataset and were provided, but the event anchors were predicted by a multi-class SVM classifier. In the first phase, the original event representation is converted to dependency trees containing event anchors and entity mentions. Each event anchor is linked to each of its arguments and is labeled with the slot name of the argument. The labeled dependency links were generated using MSTParser. In this model, the graph may contain self-referential edges due to related events sharing the same anchor. Since their re-ranking algorithm would work only on trees, they had several preprocessing steps. They removed self-referential edges and also broke structures where one argument participates in multiple events by keeping only the dependency to the event that appears first in text. Also, all events with same types anchored on the same anchor phrase was unified. After this phase, the resulting dependency structure is int he form of a tree. The MSTParser used features that were edge factored - that is, the features are extracted based on the features of end points of the edge and that of the edge itself. Everytime the MSTParser was run, it finds the highest scoring tree that incorporates the global properties that included event path (path from each node in the event tree upto the root), event frames (event anchors with all their arguments and argument slot names). Since their approach is not restricted by sentence boundaries, it could be extended to work on entire documents.


