In this section we first do general error analysis, then we talk about results of each task in detail.

{\bf Lexical vs Non-lexical features-} Using lexical features helps to capture words or phrases that are repeating. At the same time, using features from the parse tree and dependency tree, including paths, helps us to identify structural traits. Since our training data was limited, using lexical features alone wasn't very helpful because most of the words and phrases did not repeat. At the same time, adding many features based on path resulted in high precision but low recall. So, we've adopted a mix of both lexical and non-lexical features.

{\bf Annotation errors-} There were many instances when we thought the annotations were not correct. For example, events like 'transposition' and 'duplication' were not marked as a triggers in some sentences, maybe, because those were not important events according to the annotator. Also, while marking phrases that constitute an entity, there were many instances when multiple annotations were possible. This raises an import concern with all manually annotated corpora - that choices of annotations are subjective.

{\bf Parser errors-} Since many of the features that we used are based on the parses generated for a sentence, any mistakes by the parser would automatically result in misclassifications of event and entities. A typical example is problems arising due to PP attachment of phrases that were marked as entities.

\subsection{Event trigger prediction}
{\bf Verb Nominalizations-} Most of the event triggers are verbs which makes it easy to achieve F1 scores of around 0.6 without many features. But, including features to classify nominalized verbs that are event triggers is a hard task. Especially because there is no single source of word nominalizations that is perfect. We tried NomLex and NomBank, which are collections of nominalizations. NomLex proved useful and the classification accuracy improved by around 2-3\%. But, since NomLex was not an extensive dictionary, some nominalizations were still misclassified. Using WordNet derivations helped us to counter this and to improve the performance by another 2\%. After including the features for word nominalizations, the model now identifies event triggers like 'effect', 'change', 'impact', 'degradation', 'concentration' etc. as event triggers. At the same time, there are many instances when nouns get tagged as event triggers even when they were not event triggers, for instance 'damage' was tagged as trigger from the phrase 'strand containing the damage'. Also, more complex nominalizations like 'synthesis', 'bronchitis' etc. were missed. This could be fixed by including gazetteers from biological or medical domain, but that would make our models very specific to that domain.

{\bf Iterative Optimization-} The iterative optimization algorithm we developed was quite efficient in correcting some of the mistakes made by the basic trigger prediction algorithm. As an example, in the sentence, {\em The original cell produces a copy of its chromosome and surrounds it with a tough multilayered structure, forming the endospore.}, 'copy' was predicted as an event trigger, possibly because of strong lexical features. But, this gets corrected with the iterative algorithm as it does not have any child entities in the dependency tree. Similarly, in the sentence, {\em Each gene on one homolog is aligned precisely with the corresponding gene on the other homolog.}, 'aligned' was not initially predicted as an event trigger, but the iterative algorithm predicts it as a trigger.

We feel that including word clustering features that cluster together words that are semantically close would help us solve the problems arising with sparsity of labeled data. It is also interesting to note that the models performed well in the trigger identification task on a general dataset and it matched the intuition that our models should work for general datasets since we are not using any domain specific features or dictionaries.

\subsection{Entity prediction}
The task of argument prediction is a much harder task as evident from the lower F1 scores, mainly because of two reasons. Firstly, entities that are arguments to event triggers are phrases and marking boundaries of entities are subjective. We partly overcame this difficulty by modeling entities as parse tree nodes and devising a dynamic program for enforcing non overlapping constraints. The dynamic programming approach gave us a boost of around 4\% in F1 score. Secondly, since we are now scoring event-entity combinations, identifying correct triggers to connect the entity to is a hard problem, especially in the cases when an entity is shared between two triggers or when the entities and trigger words are far apart in the sentence. We tried to overcome this by introducing features based on dependency parse structure of sentences as well.

The model to predict entities related to an event trigger did not perform well on the general dataset. We found that news headlines and articles on the web have a lot of named entities as arguments to event triggers, but, our model did not have any features to identify named entities. Once we include features to capture named entities, the performance of the model should be comparable to the results on our main dataset.

\subsection{Semantic role labeling}
\begin{enumerate}
\item {\bf Domain Roles and Annotations-} The roles for our domain include 'Agent', 'Result', 'Location', 'Origin', 'Destination', and 'Theme'. These roles have overlapping semantic interpretations i.e. Location, Origin, Destination have very subtle semantic differences. Similarly, 'Result' and 'Theme' are often interchangeable in many scenarios. As a result, the annotations generated are subjective and too sparse to tease apart the subtle semantic differences during training.

\item {\bf Parse Tree Node labeling-} Since this is the approach we chose across all our tasks, another error we commonly see is as follows. Consider (NP (NNS gametes)) in the sentence: {\em 'Subsequently, the haploid organism carries out further mitoses, producing the cells that develop into gametes'}. Now we have the label (NP (NNS gametes)) as 'Result' in gold when actually the node (NNS gametes)  is predicted as 'Result' in our model. These prediction errors are caused by the usage of parse tree nodes. We observe that non-overlapping constraints and re-ranking resolves these issues to a significant extent.
\item {\bf Choice of k-} We experimented with different values of k in our top-k re-ranking model. The tradeoff is between space optimal calculations versus considering a large set of labelings for the best label. For our dataset, k=5 works well in terms of performance.
\end{enumerate}

From our analysis on Semantic Role labeling we conclude that:
\begin{enumerate}
\item Having a smaller set of possible roles which are more mutually distinct would be very likely to improve the results.
\item A joint model for Semantic role labeling and entity prediction with the re-ranking approach could be attempted since these two tasks are closely tied together.
\end{enumerate}