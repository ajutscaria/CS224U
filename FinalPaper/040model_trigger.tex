\subsection{Event trigger prediction}
As we mentioned earlier, events are represented as pre-terminal nodes in the parse tree of a sentence. Let $N_{pt}$ be the set of all $n_{pt}$ pre-terminal parse tree nodes $\{e_1, e_2, ... e_{n_{pt}}\}$. The trigger prediction algorithm generates a list of triggers in the sentence, $T : N_{pt} \rightarrow \{0,1\}^{n_{pt}}$ that indicates if a word is a trigger or not. Formally, our goal is to maximize the likelihood of the triggers $P(T | x)$ given the sentence, where $x$ is the tokens within the sentence.

The baseline model predicted every pre-terminal node whose part-of-speech tag started with 'VB' to be an event trigger. As the next step, we designed a MaxEnt model that trained on the annotated samples using several lexical, dependency tree based and parse tree based features. The mode is local, in the sense that it predicts if a pre-terminal node is an event trigger independent of other predictions. Hence, \\

$P_{T}(T | x) = \prod_{e_{i}\in N_{pt}} P_{T}(e_{i} | x) $

The features we started with for event trigger prediction were part-of-speech tag of the word, its lemma, the path from root to the node in parse tree and the label of the outgoing edges from the node in the dependency graph. On doing initial error analysis, we found that our classifier failed to identify all nominalized verb forms as event triggers. We used NomLex and NomBank, which are collections of nominalizations, to include a feature that indicates nominalization. NomLex proved useful and the classification accuracy improved by around 2-3\%. But, since NomLex was not an extensive dictionary, some other nominalizations were still misclassified. So, we also used WordNet derivations to replace any nominalized verbs with its actual verb form and this improved the performance by another 2\%.