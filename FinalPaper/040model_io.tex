\subsection{Iterative Optimization Algorithm}
One of the drawbacks of our approach to event trigger and argument prediction was that the models were independent. As a result, any errors made in trigger prediction will cascade and affect the argument identification phase. For instance, if an event trigger is not predicted, we are going to miss all the argument predictions for the trigger as well. At the same time, any incorrect event triggers predicted may result in lots of event-argument pairs being predicted that do not exist. Even after we used NomLex to find nominalizations, our classifier missed predicting some of the triggers. But, if we knew that there were entities that were children of the word in the dependency graph, then it gives more evidence that the word is an event trigger. At the same time if there are words that we might have predicted as triggers that do not have any entities as children in the dependency tree, they are less likely to be even triggers. Hence, we hypothesized that knowledge about entities in the sentence can help in event trigger prediction as we can use more features from the dependency tree. To test this, we used the gold entities from a sentence to add more features for event trigger prediction and we found that it gave us a boost in F1 score from 0.68 to 0.82. Since gold entities are not available during the test phase, we designed a separate model that predicts entities in a sentence independent of the triggers. As before, entities are sub-trees under a parse tree node and the model predicts the probability of a node to be an entity or not. We used the head word of the span covered by the node, its POS tag, its parents POS tag, the path to the node in the parse tree and the CFG rule expansion of its parent as features. We used the same dynamic programming approach for non-overlapping constraints of entities.

\begin{figure}[t]
	\includegraphics[width=1\columnwidth]{Images/IO}
	\caption{Stages in iterative optimization algorithm}
	\label{fig:iosteps}
\end{figure}

The different stages involved in the iterative optimization algorithm is depicted in Figure~\ref{fig:iosteps}. We use the event triggers predicted to predict its arguments. Now, we combine the set of entities predicted as arguments, with those predicted by the independent argument prediction model to create the set of all entities and is used in trigger prediction. This model predicts the most likely labeling $T$ that maximizes

$P(T | x, A) = \prod_{e_{i}\in N_{pt}} P(l_{i} | x, A) $,\\
 where $l_i$ is the labeling of $e_i$ to $TRIG$ or $NONE$ according to the mapping T.

Once this model predicts the event triggers, we use the same model $P(A | e,x)$ to predict arguments associated with each event trigger that satisfy the non-overlapping constraint. We repeat the iterative trigger and argument prediction till there is no improvement in F1 score.

The mode $P(T | x, A)$ uses the path to entities in parse tree, path length to the nearest entity, the actual path to nearest entity and the number of words between closest entity before and after the node as features.
