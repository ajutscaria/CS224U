In this project, we combine the learnings from the different methodologies to build a model that can be used for event and entity extraction with classes that are not specific to any domain. Even though our dataset is based on paragraphs from a biology textbook, we believe our models would generalize well to deal with more general content as our features and event/entity classes are not tied to the biological domain in any way. The key modeling decisions are as follows:

\begin{itemize}
\item We model events and entities as nodes in the constituency tree. Each sentence is assumed to be independent of each other as far as entity-event relationships are concerned. Events are denoted by their trigger word and are hence pre-terminals in the parse tree. Entities are denoted by a parse tree-node covering a sub-tree that spans over the whole text of the entity. In some cases when there is no single node that covers the entire entity (mostly because of parser errors, for e.g., PP attachment), we use an approximation by repeatedly removing tokens from the end or beginning of the span of text to identify a node that covers it. We manually verified that this heuristic works well in practice and results in entities that convey almost the full meaning of original span, and are well-formed.
\item We also use features based on the dependency parse in conjunction with the constituency parse, since it contains information about the dependencies between tokens. This is done by identifying the position of the headword of an entity or event from the dependency tree and analyzing the relations.
\item We handle task 1 as an independent classification task, while tasks 2 and 3 are designed as a joint classification task. For all classification tasks, we use maximum entropy model based on an implementation of L-BFGS for Quasi Newton unconstrained minimization.
\end{itemize}