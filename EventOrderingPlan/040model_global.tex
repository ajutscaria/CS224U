\subsection{Modeling global dependencies}
The main function of the global constraints is to ensure more probable assignments of all the event-event relations in addition to resolving potentially inconsistent decisions generated by the local models. The global constraints help satisfy consistent assignments by encouraging the local assignments to agree with paragraph level properties that are global. We have two kinds of global potentials. The ones for hard constraints that are defined as:

\begin{equation}
\theta_f(y_{f-p}) = 
\left\{\begin{matrix}
0 & if property holds\\ 
-\infty & otherwise
\end{matrix}\right.
\end{equation}

and the ones for soft constraints defined as:
\begin{equation}
\theta_f(y_{f-p}) = 
\left\{\begin{matrix}
\alpha_p & if property holds\\ 
0 & otherwise
\end{matrix}\right.
\end{equation}


where $f-p$ is the index set of variables over which the potential is defined.

We use four global consistency potentials:

\begin{enumerate}
\item {\em Next event potential} is a soft constraint and is applied for each event. It is used to penalize if an event has more than one {\em NextEvent.}
\item {\em Super event potential} is a soft constraint and is applied for each event. It is used to penalize if event has more than one {\em SuperEvent.}
\item {\em Connected component potential} is applied for the full network to ensure that all events in the paragraph are form a single connected component of undirected edges. This is a hard constraint.
\item {\em Consistency potential} is applied for each pair of events and is used to ensure that there are no inconsistent assignments. For instance, two events cannot be both {\em NextEvent} of each other.
\end{enumerate}

{\bf Dual Decomposition} The global dependencies are very important as they ensure consistent and more likely global assignments. But, at the same time, enforcing these dependencies complicates the inference by a great extend as now we have to consider all possible assignments of all variables in the factors generated by the dependencies. Hence, we propose an approach based on dual-decomposition, which is an inference technique that helps to compute a tighet upper bound on the original MAP objective efficiently which preserving all the original dependencies. As a first step, we modify the MAP equation to include the local potentials for the event pairs.

\begin{equation}
MAP(\theta) = \max_{y} \sum_{j \in J} \theta_j(r_j) + \sum_{f \in F} \theta_f(r_f)
\end{equation}

where the set of unobserved variables are denoted by $J$ and each of the variables is denoted by $r_j$. The dual problem is now defined as:

\begin{multline}
\min_{\delta} L(\delta), L(\delta) = \sum_{j \in J} \max_{r_j}[\theta_j(r_j) + \sum_{f:j \in f} \delta_{fj}(r_j)]\\ +
 \sum_{f \in F} \max_{r_f}[\theta_f(r_f) - \sum_{j \in f} \delta_{fj}(r_j)]
\end{multline}

where for every $f \in F$ and $j \in f, \delta_{fj}$ is a vector of Lagrange multipliers with an entry for each possible assignment of $r_j$. Our goal is to find the tightest upper bound for the dual objective $L(\delta)$. This can be done efficiently by using subgradient descent algorithm.

{\bf Inference algorithm} The dual decomposition algorithm and arg max computation algorithm we use are similar to Reichart and Barzilay.

{\bf Maximizing Individual Potentials} For the local potentials $rl_j^k$, the maximizing assignments are found by decoding the likelihoods of the assignments and picking the most likely assignment from the sorted list of likelihoods from the MaxEnt model.

The global potentials are harder to compute. First step is to compute the {\em minimum-message assignment (MMA)} which minimizes the message sum. If this assignment is consistent with the potential property, then this is the best assignment. If not, we compute the {\em property-respecting assignment (PRA)}, which gives the lowest message sum under which the potential property holds. The best of MMA and PRA is selected by our algorithm.

MMA is the minimum message assignment of each unobserved variable in isolation. For PRA, we have to evalute over a very large number of assignments. We begin with the MMA and try to generate all possible combinations that satisfy a specific potential property. 

\begin{itemize}
\item {\em Next event potential} For each event, we need to restrict the number of next events to at most 1. So, if there are more than one next-events for an event according to the local models, then we have to ensure that the constraint is satisfied by finding the best next-event to keep. This can be done by generating all options to keep exactly one of the events tagged as next events while assigning the next-best option for the other events. This is followed by picking the best event to keep as the next-event.
\item {\em Super event potential} This constraint can be done in a similar way to 'Next Event Potential'.
\item {\em Connected component potential} This can be modeled by Kruskal's algorithm. The weight of an edge between two nodes is the score for the best non-NONE labelingbetween the two events. The algorithm could continue till there are no more positive edges to be added or the graph is not connected yet. 
\item {\em Consistency potential} This constraint can be satisfied by evaluating the relation between each pair of events and making sure that the assignments are consistent. For instance, two events can both not be next events of each other. To resolve inconsistencies, we pick the highest scoring consistent assignment for the pair by taking the second-best option for either. Since this would result in a change in labeling of one of the relations, we have to ensure that the new scheme is also consistent. This may result in cascading changes.
\end{itemize}


