Health care in the United States is provided by many distinct organizations. Health care facilities are largely owned and operated by private sector businesses. Health insurance for public sector employees is primarily provided by the government.
