\section{Joint Model for Process Extraction}

Given a paragraph $\bx$ and a trigger set $\sT$ we wish to extract all event-event relations $E$. Similar to Do et al. \shortcite{Do12} our model consists of a local pairwise classifier and global constraints. We first introduce a classifier that is based on features from previous work (Section~\ref{subsec:pairwise}). Next, we describe novel features specific for process extraction (Section~\ref{subsec:pairwise-novel}). Last, we incorporate global constraints into our model in an ILP formulation (Section~\ref{subsec:global}).

\subsection{Local pairwise classifier} \label{subsec:pairwise}

The pairwise classifier predicts relations between all event mention pairs (represented by their triggers). Since some of the relations in $\sR$ are directed, we must predict also the direction of these relations. We do this by expanding $\sR$ to include the reverse of four directed relations: \textsc{Prev}-\textsc{Next},  \textsc{Super}- \textsc{Sub}, \textsc{Causes}-\textsc{Caused}, \textsc{Enables}-\textsc{Enabled}. After adding \textsc{None} to indicate no relation, $\sR$ contains 11 relations. Our classifier is a function $f:\sT \times \sT \rightarrow \sR$. Let $n$ be the number of triggers in a process description, and $t_i$ be the i'th trigger appearing in the description, since $f(t_i,t_j)$ completely determines $f(t_j,t_i)$ it suffices to consider only pairs such that $i<j$. Note that in this new definition of $\sR$ the process graph $\sP=(V,E)$ is undirected.

Table~\ref{tab:features} describes features from previous work~\cite{Chambers08,Do12} extracted for a trigger pair $(t_i,t_j)$. Some features were omitted since they did not yield improvement in performance on a development set, or they require gold annotations provided in TimeBank, which we do not have. To reduce sparseness, we convert nominalizations into their verbal forms when computing word lemmas, using WordNet's \cite{Fellbaum1998} derivation links.

\begin{table}[t]
{\footnotesize
\hfill{}
\begin{tabular}{|p{2.4cm}|p{4.7cm}|}
\hline
\textbf{Feature} &\textbf{Description}\\
\hline
 POS & Pair of POS tags \\
Lemma & Pair of lemmas \\
Prep$^*$ & Preposition lexeme, if in a prepositional phrase \\
Words between  & For adjacent triggers, content words between triggers \\
Temp. between & For adjacent triggers, temporal connectives (from a small list) between triggers \\
Adjacency & Whether two triggers are adjacent \\
\# Sent. & Quantized number of sentences between triggers \\
\# Word. & Quantized number of words between triggers \\
LCA & Least common ancestor on constituency tree, if exists \\
Dominates$^*$ & Whether one trigger dominates other \\
Share & Whether triggers share a child on dependency tree \\
\hline
\end{tabular}}
\hfill{}
\caption{Features extracted for a trigger pair $(t_i,t_j)$. Asteriks (*) indicate features that are duplicated, once for each trigger.}
\label{tab:features}
\end{table}

\subsection{Classifier extensions} \label{subsec:pairwise-novel}

A central source of information for extracting event-event relations from text are \emph{connectives} such as \emph{after}, \emph{during}, etc. However, there is variability in the occurrence of these connectives. Consider the following two sentences (connectives in bold, triggers in italics):

\begin{enumerate}[itemsep=0pt,topsep=0pt] 
\item \footnotesize \textbf{Because} alleles are \emph{exchanged} during \emph{gene flow}, genetic differences are \emph{reduced}. \label{sent:1}
\item \footnotesize During \emph{gene flow}, alleles are \emph{exchanged}, and \textbf{so} genetic differences are \emph{reduced}. \label{sent:2}
\end{enumerate}

Both sentences express the relation $(\mbox{\emph{exchanged}},\mbox{\emph{reduced}},\mbox{\textsc{Causes}})$, but the connective used is different, its linear position with respect to the triggers is different, and in sentence~\ref{sent:1} the trigger \emph{gene flow} intervenes between \emph{exchanged} and \emph{reduced}. Since our data set is very small, we would like to identify the triggers related to each connective, and share features between such sentences. We do this using the dependency structure and a clustering of connectives.

[Details on clustering and syntactic feature implementation probably using example above, maybe explain using "marker", then say something that we do something similar for things that are under PP but do not describe for simplicity, and say that for advmod we only use the clustering]

We further extend our features to handle the rich relation set necessary for process extraction. Processes often begin with a trigger for an event that includes subsequent triggers, e.g., ``The \emph{Calvin cycle} begins by \emph{incorporating}...". Thus, we add a feature for $t_i$ indicating whether $i=1$ and $t_i$  is a noun. We also add two features targeted at the relation \textsc{Same}: one indicating whether the lemmas of $t_i$ and $t_j$ are equal, and another specifying the determiner of $t_j$, if it exists. Intuition is that certain determiners indicate that the event triggered had already been mentioned, e.g., the determiner \emph{this} hints a \textsc{Same} relation in ``The next steps \emph{decompose} citrate back to oxaloacetate. This \emph{regeneration} makes...". Last, we add as a feature the dependency path between $t_i$ and $t_j$, if it exists, e.g., the feature $\xrightarrow{\scriptscriptstyle dobj} \xrightarrow{\scriptscriptstyle rcmod}$ between \emph{produces} and \emph{divide} will fire in ``meiosis produces cells that divide...".

For our pairwise classifier, we train a maximum entropy classifier that provides a probability $p_{ijr}$ for every trigger pair $(t_i,t_j)$ and relation $r$. Hence, $f(t_i,t_j)= \arg\max_r p_{ijr}$.

\subsection{Global Constraints} \label{subsec:global}

Naturally, a pairwise classifier can result in a process structure that violates global properties (see Section~\ref{sec:process}). Figure~\ref{} shows in black edges the predictions of our local classifier, which result in the trigger [HERE WILL BE AN EXAMPLE FOR BAD LOCAL DECISIONS]. In this section we incorporate into our model constraints that result in a coherent global process structure.

Let $\theta_{ijr}$ be a score for the relation $r$ and the triggers $(t_i,t_j)$ (e.g, $\theta_{ijr}=\log p_{ijr}$), and $y_{ijr}$ be the corresponding indicator. Our goal is to find an assignment for the indicators $\by=\{y_{ijr} \ | \ 1 \leq i < j \leq n, r \in \sR \}$. With no global constraints this can be formulated as the following ILP:

\begin{align}
\argmax_{\by} \sum_{ijr} \theta_{ijr} y_{ijr} \\
\mbox{s.t.} \forall_{i,j} \sum_r y_{ijr}=1 \nonumber
% \forall_{i,j,r} \ y_{ijr} \in \{0,1\} \nonumber
\end{align}

\noindent where the constraint ensures each trigger pair is assigned exactly one relation. We now describe constraints that result in a process that is connected, ``chain-like", and has reasonable relation triangles.

\paragraph{Connectivity} 
Our formulation for enforcing connectivity is a minor variation on the one suggested by Martins et al. \shortcite{Martins09} for dependency parsing. In our setup, we want $\sP$ to be a connected undirected graph, and not a directed tree. However, an undirected graph is connected iff there is a directed tree embedded in it, and thus the resulting formulation is almost identical. This formulation is based on flow constraints that ensure that there is a path from a designated root in the graph to all other nodes.

 Let $\bar{\sR}$ be the set $\sR \setminus \mbox{\textsc{None}}$. An edge $(t_i,t_j)$ is in $E$ if $y_{ij}=\sum_{r \in \bar{\sR}} y_{ijr}=1$. For each variable $y_{ij}$ we define two auxiliary binary variables $z_{ij}$ and $z_{ji}$ that correspond to the directed edges in the embedded tree. We tie each auxiliary variable to its corresponding ILP variable by ensuring  $z_{ij}$ and $z_{ji}$ are active only if $y_{ij}$ is active:
 
\begin{align}
\forall_{i<j} \ z_{ij}<y_{ij}, z_{ji} < y_{ij}
\end{align}

Next, we add constraints that enforce that the graph structure induced by the auxiliary variables results in a tree rooted in node $1$. For that we need for every $i \neq j$ a flow variable $\phi_{ij}$ which specifies the amount of flow on the directed edge $z_{ij}$.

\begin{align}
\small &\sum_{i} z_{i1} =0, \forall_{j \neq 1} \sum_{i} z_{ij}=1 \label{eq:oneparent} \\ 
&\sum_{i} \phi_{1i}=n-1 \label{eq:rootflow} \\ 
&\forall_{j} \ \sum_{i} \phi_{ij} - \sum_{k} \phi_{jk}=1 \label{eq:flow} \\
&\forall_{i \neq j} \ \phi_{ij} \leq n \cdot z_{ij} \label{eq:tie} 
\end{align}

Equation~\ref{eq:oneparent} says that all nodes in the graph have exactly one parent, except for the root that has no parents. Equation~\ref{eq:rootflow} ensures that the outgoing flow from the root is $n-1$, and Equation~\ref{eq:flow} states that each of the other $n-1$ nodes consumes exactly one flow unit. Last, Equation~\ref{eq:tie} ties the auxiliary variables to the flow variables, making sure that flow occurs only on edges. The combination of these constraints guarantees that the graph induced by the variables $z_{ij}$ is a directed tree and consequently the graph induced by our variables $\by$ is connected.

\paragraph{Chain structure} 
A connected graph where the degree of all nodes is $\leq 2$ is a chain. Table~\ref{tab:degree} presents nodes' degree and demonstrates that indeed process graphs are close to being chains. The following constraint bounds nodes' degree by 2:

\begin{align}
\forall_j \sum_{i<j} y_{ij} + \sum_{j<k} y_{jk} \leq 2
\end{align}

Since graph structures are not always chains we add this as a soft constraint, that is, we penalize the objective for each node with degree $>2$. Thus, our modified objective function is $\sum_{ijr} \theta_{ijr} y_{ijr} + \sum_{k \in \sK} \alpha_k C_k$, where $\sK$ is the set of soft constraints, $\alpha_k$ is the penalty, and $C_k$ indicates whether a constraint is violated. We tune the parameters $\alpha_k$ on a development set, as explained in Section~\ref{sec:experiment}.

\paragraph{Relation triangles} 
A triangle is a 3-tuple of relations $(f(t_i,t_j),f(t_j,t_k),f(t_i,t_k))$. Clearly, some triangles are impossible while others are quite common. In order to look for triangles that could potentially improve process extraction we counted how many time each possible triangle occurs in both the training data and the output of our pairwise classifier, and focused on those for which the classifier and the gold standard disagree. We are interested in triangles that never occur in the training data but are predicted by the classifier, and triangles that are frequent in the gold standard but do not appear in the classifier output. Table~\ref{tab:constraintformulation} illustrates the triangles found and Equations 8-14 provide the corresponding ILP formulation. Soft constraints are incorporated by defining a penalty/reward $\alpha_k$ for each triangle structure and expanding the set $\sK$ accordingly. 
\begin{enumerate}[itemsep=0pt,topsep=0pt] 
\item \textsc{Same} transitivity: The transitivity of \textsc{Same} (co-reference) has been used in past work \cite{Finkel08} and we incorporate it as a soft constraint (Equation 8).
\item \textsc{Cotemp} transitivity:  If $t_i$ is co-temporal with $t_j$ and $t_j$ is co-temporal with $t_k$, then usually $t_i$ and $t_k$ are either co-temporal or denote the same event (soft constraint in Equation 9).
\item \textsc{Cause}-\textsc{Cotemp}: If $t_i$ causes both $t_j$ and $t_k$, then often $t_j$ and $t_k$ are co-temporal. E.g, in ``\emph{genetic drift} has led to a \emph{loss} of genetic variation and an \emph{increase} in the frequency of harmful alleles", a single event causes two subsequent events that occur simultaneously (soft constraint in Equation 10).
\item \textsc{Same} contradiction: if $t_i$ is the same event as  $t_j$, then their temporal ordering with respect to a third trigger $t_k$ may result in a contradiction, e.g., if $t_i$ is before $t_k$, but $t_j$ is after $t_k$. We define 5 temporal categories that generate $5 \choose 2$ possible contradictions, but for brevity present just one representative hard constraint (Equations 11 - 13). This constraint ensures that if $f(t_j,t_k) = SAME$, then $t_i$ cannot occur before $t_j$ and after $t_k$. Note that this constraint depends on co-reference and temporal relations being predicted jointly.
\item \textsc{Prev}: As mentioned (Section~\ref{sec:model}), if $t_i$ is immediately before $t_j$, and $t_j$ is immediately before $t_k$, then it can not be that $t_i$ is immediately before $t_k$ (hard constraint in Equation 14).
\end{enumerate}.

\begin{table}[t]
{\footnotesize
\hfill{}
\begin{tabular}{|p{2cm}|p{5.1cm}|}
\hline
\textbf{Name} &\textbf{Constraint formulation}\\
\hline
SAME transitivity &  \begin{align}
y_{ijSAME} + y_{jkSAME} - y_{ikSAME} \leq 1
\end{align}\\

COTEMP transitivity &
\begin{align}
y_{ijCOTEMP} + y_{jkCOTEMP} - y_{ikCOTEMP} - y_{ikSAME} \leq 1
\end{align}\\
\\

CAUSE - COTEMP &
\begin{align}
y_{ijCAUSES} + y_{jkCOTEMP} - y_{ikCAUSES} \leq 1
\end{align}\\
\\

SAME contradiction &
\begin{align}
c_{ijPAST} = y_{ijPREV} + y_{ijCAUSES} + y_{ijENABLES} \\
c_{ijFUTURE} = y_{ijNEXT} + y_{ijCAUSED} + y_{ijENABLED} \\
c_{ijPAST} + y_{jkSAME} + c_{ikFUTURE} \leq 2
\end{align}\\
\\

PREV & 
\begin{align}
y_{ijPREV} + y_{jkPREV} - y_{ikNONE} \leq 1
\end{align}\\
\\


\hline
\end{tabular}}
\hfill{}
\caption{Constraint formulation for event tringles.}
\label{tab:constraintformulation}
\end{table}

We used the Gurobi optimization package\footnote{\url{www.gurobi.com}} to find an exact solution for our ILP, which contains $O(n^2r)$ variables and $O(n^3)$ constraints. We have also developed an equivalent formulation amenable to dual decomposition \cite{Reichart12}, which is a faster approximation method, but practically found that solving the problem exactly with Gurobi is quite fast (average/median time per process: 0.294 sec/ 0.152 sec).



