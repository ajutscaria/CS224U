\section{Joint Model for Process Extraction}

Given a paragraph $\bx$ and a trigger set $\sT$ we wish to extract all event-event relations $E$. Similar to Do et al. \shortcite{Do12} our model consists of a local pairwise classifier and global constraints. We first introduce a classifier that is based on features from previous work (Section~\ref{subsec:pairwise}). Next, we describe novel features specific for process extraction (Section~\ref{subsec:pairwise-novel}). Last, we incorporate global constraints into our model in an ILP formulation (Section~\ref{subsec:global}).

\subsection{Local pairwise classifier} \label{subsec:pairwise}

The pairwise classifier predicts relations between all event mention pairs (represented by their triggers). Since some of the relations in $\sR$ are directed, we must predict also the direction of these relations. We do this by expanding $\sR$ to include the reverse of four directed relations: \textsc{Prev}-\textsc{Next},  \textsc{Super}- \textsc{Sub}, \textsc{Causes}-\textsc{Caused}, \textsc{Enables}-\textsc{Enabled}. After adding \textsc{None} to indicate no relation, $\sR$ contains 11 relations. Our classifier is a function $f:\sT \times \sT \rightarrow \sR$. Let $n$ be the number of triggers in a process description, and $t_i$ be the i'th trigger appearing in the description, since $f(t_i,t_j)$ completely determines $f(t_j,t_i)$ it suffices to consider only pairs such that $i<j$. Note that in this new definition of $\sR$ the process graph $\sP=(V,E)$ is undirected.

Table~\ref{tab:features} describes features from previous work~\cite{Chambers08,Do12} extracted for a trigger pair $(t_i,t_j)$. Some features were omitted since they did not yield improvement in performance on a development set, or they require gold annotations provided in TimeBank, which we do not have. To reduce sparseness, we convert nominalizations into their verbal forms when computing word lemmas, using WordNet's \cite{Fellbaum1998} derivation links.

\begin{table}[t]
{\footnotesize
\hfill{}
\begin{tabular}{|p{2.4cm}|p{4.7cm}|}
\hline
\textbf{Feature} &\textbf{Description}\\
\hline
 POS & Pair of POS tags \\
Lemma & Pair of lemmas \\
Prep$^*$ & Preposition lexeme, if in a prepositional phrase \\
Words between  & For adjacent triggers, content words between triggers \\
Temp. between & For adjacent triggers, temporal connectives (from a small list) between triggers \\
Adjacency & Whether two triggers are adjacent \\
\# Sent. & Quantized number of sentences between triggers \\
\# Word. & Quantized number of words between triggers \\
LCA & Least common ancestor on constituency tree, if exists \\
Dominates$^*$ & Whether one trigger dominates other \\
Share & Whether triggers share a child on dependency tree \\
\hline
\end{tabular}}
\hfill{}
\caption{Features extracted for a trigger pair $(t_i,t_j)$. Asteriks (*) indicate features that are duplicated, once for each trigger.}
\label{tab:features}
\end{table}

\subsection{Classifier extensions} \label{subsec:pairwise-novel}

A central source of information for extracting event-event relations from text are \emph{connectives} such as \emph{after}, \emph{during}, etc. However, there is variability in the occurrence of these connectives. Consider the following two sentences (connectives in bold, triggers in italics):

\begin{enumerate}[itemsep=0pt,topsep=0pt] 
\item \footnotesize \textbf{Because} alleles are \emph{exchanged} during \emph{gene flow}, genetic differences are \emph{reduced}. \label{sent:1}
\item \footnotesize During \emph{gene flow}, alleles are \emph{exchanged}, and genetic differences are \textbf{hence} \emph{reduced}. \label{sent:2}
\end{enumerate}

Both sentences express the relation $(\mbox{\emph{exchanged}},\mbox{\emph{reduced}},\mbox{\textsc{Causes}})$, but the connective used is different, its linear position with respect to the triggers is different, and in sentence~\ref{sent:1} the trigger \emph{gene flow} intervenes between \emph{exchanged} and \emph{reduced}. Since our data set is very small, we would like to identify the triggers related to each connective, and share features between such sentences. We do this using the syntactic structure and a clustering of connectives.

Sentence~\ref{sent:1} presents a typical case where by walking up the dependency tree from the marker \emph{because} we can find the triggers related by this marker: $\mbox{\emph{because}}\xleftarrow{\scriptscriptstyle mark}\mbox{\emph{exchanged}}\xleftarrow{\scriptscriptstyle advcl}\mbox{\emph{reduced}}$. Whenever a trigger is the head of an adverbial clause and marked by a \emph{mark} dependency label, we walk on the dependency tree and look for a trigger in the main clause that is closest to the root (or the root itself in this example). 
By utilizing the syntactic structure we can correctly ignore the trigger \emph{gene flow} that is linearly closer to the trigger \emph{exchanged}. After locating the relevant pair of triggers, we reduce sparseness utilizing a hand-made clustering of 30 connectives that maps words such as \emph{because} and \emph{since} to a ``causality" cluster and fire a feature for this cluster. We perform a similar procedure whenever a trigger is part of a prepositional phrase (imagine sentence~\ref{sent:1} starting with ``due to allele exchange during gene flow...") by walking up the constituency tree, but we omit details here for brevity. In sentence~\ref{sent:2}, the connective \emph{hence} is an adverbial modifier of the trigger \emph{reduced}. We look up the cluster for the connective \emph{hence} and fire the same feature in this sentence as well for the adjacent triggers \emph{exchanged} and \emph{reduced}.

%One of the signals we use to identify connectives between event triggers is to use markers in dependency graph of the sentence. A marker of an adverbial clausal complement (advcl) is the word introducing it and carries information as to how triggers are related. We also use adverbial modifier relations that modifies the meaning of the word as another signal. In sentence 1 in the above example, \emph{because} acts as a marker between the triggers \emph{exchanged} and  \emph{reduced}. But, in sentence 2, \emph{hence} is an adverbial modifier to \emph{reduced}. We see that even though the sentences have the same semantics, the dependency relations ensued is very different. In fact, \emph{because} and \emph{hence} indicates that one trigger causes the other, but, $because X, Y$ translates to $X hence Y$. Hence, we would like the same set of features to be fired for the pair of triggers for both the sentences. To this end, we cluster the different marker and advmod relations based on the relation type it indicates. While appearing in the text between the event triggers, \emph{because} indicates $CAUSED$, whereas \emph{hence} indicates $CAUSES$. But, when \emph{because} appears before the first trigger, the causal relation is inverted, and as a result, it indicates $CAUSES$ as well. We extract a marker to indicate a relation between a pair of triggers if one is related to the other by an adverbial clause and the former has a marker attached to it. For advmod, we extract the relation if the two triggers appear consecutive in text and the latter has an advmod attached to it. Instead of using the actual marker or adverbial modifier, we insert the cluster name as the feature. We see that for both the sentences, the same cluster feature is fired. We also include connectives based on prepositional phrases, but will not go into details for brevity.

We further extend our features to handle the rich relation set necessary for process extraction. Processes often begin with a trigger for an event that includes subsequent triggers, e.g., ``The \emph{Calvin cycle} begins by \emph{incorporating}...". Thus, we add a feature for $t_i$ indicating whether $i=1$ and $t_i$  is a noun. We also add two features targeted at the relation \textsc{Same}: one indicating whether the lemmas of $t_i$ and $t_j$ are same, and another specifying the determiner of $t_j$, if it exists. Intuition is that certain determiners indicate that the event triggered had already been mentioned, e.g., the determiner \emph{this} hints a \textsc{Same} relation in ``The next steps \emph{decompose} citrate back to oxaloacetate. This \emph{regeneration} makes...". Last, we add as a feature the dependency path between $t_i$ and $t_j$, if it exists, e.g., the feature $\xrightarrow{\scriptscriptstyle dobj} \xrightarrow{\scriptscriptstyle rcmod}$ between \emph{produces} and \emph{divide} will fire in ``meiosis produces cells that divide...". In Section~\ref{subsec:results} we will empirically show that our extension to the local classifier substantially improves performance

For our pairwise classifier, we train a maximum entropy classifier that provides a probability $p_{ijr}$ for every trigger pair $(t_i,t_j)$ and relation $r$. Hence, $f(t_i,t_j)= \arg\max_r p_{ijr}$.

\subsection{Global Constraints} \label{subsec:global}

Naturally, a pairwise classifier can result in a process structure that violates global properties (see Section~\ref{sec:process}). Figure~\ref{fig:graph} shows in black edges the predictions of our local classifier, which result in the trigger \emph{passed} being isolated from the rest of the process [\textcolor{red}{TODO} NEED BETTER EXAMPLE WITH INTERESTING EXPLANATION]. In this section we incorporate into our model, constraints that result in a coherent global process structure.

Let $\theta_{ijr}$ be a score for the relation $r$ and the triggers $(t_i,t_j)$ (e.g, $\theta_{ijr}=\log p_{ijr}$), and $y_{ijr}$ be the corresponding indicator. Our goal is to find an assignment for the indicators $\by=\{y_{ijr} \ | \ 1 \leq i < j \leq n, r \in \sR \}$. With no global constraints this can be formulated as the following ILP:

\begin{align}
\argmax_{\by} \sum_{ijr} \theta_{ijr} y_{ijr} \\
\mbox{s.t.} \forall_{i,j} \sum_r y_{ijr}=1 \nonumber
% \forall_{i,j,r} \ y_{ijr} \in \{0,1\} \nonumber
\end{align}

\noindent where the constraint ensures each trigger pair is assigned exactly one relation. We now describe constraints that result in a process with a coherent global structure:

\paragraph{Connectivity} 
Our formulation for enforcing connectivity is a minor variation on the one suggested by Martins et al. \shortcite{Martins09} for dependency parsing. In our setup, we want $\sP$ to be a connected undirected graph, and not a directed tree. However, an undirected graph $\sP$ is connected iff there is a directed tree that is a subgraph of $\sP$ when edge directions are ignored. Thus the resulting formulation is almost identical. This formulation is based on flow constraints that ensure that there is a path from a designated root in the graph to all other nodes.

 Let $\bar{\sR}$ be the set $\sR \setminus \mbox{\textsc{None}}$. An edge $(t_i,t_j)$ is in $E$ if there is some none-\textsc{None} relation between $t_i$ and $t_j$: $y_{ij}=\sum_{r \in \bar{\sR}} y_{ijr}=1$. For each variable $y_{ij}$ we define two auxiliary binary variables $z_{ij}$ and $z_{ji}$ that correspond to edges of the directed tree that is a subgraph of $\sP$. We ensure that the edges in the tree exist also in $\sP$ by tying each auxiliary variable to its corresponding ILP variable:
 
\begin{align}
\forall_{i<j} \ z_{ij} \leq y_{ij}, z_{ji} \leq y_{ij}
\end{align}

Next, we add constraints that enforce the graph structure induced by the auxiliary variables is a tree rooted in an arbitrary node $1$ (The choice of root doesn't affect connectivity). We add for every $i \neq j$ a flow variable $\phi_{ij}$ which specifies the amount of flow on the directed edge $z_{ij}$.

\begin{align}
\small &\sum_{i} z_{i1} =0, \forall_{j \neq 1} \sum_{i} z_{ij}=1 \label{eq:oneparent} \\ 
&\sum_{i} \phi_{1i}=n-1 \label{eq:rootflow} \\ 
&\forall_{j \neq 1} \ \sum_{i} \phi_{ij} - \sum_{k} \phi_{jk}=1 \label{eq:flow} \\
&\forall_{i \neq j} \ \phi_{ij} \leq n \cdot z_{ij} \label{eq:tie} 
\end{align}

Equation~\ref{eq:oneparent} says that all nodes in the graph have exactly one parent, except for the root that has no parents. Equation~\ref{eq:rootflow} ensures that the outgoing flow from the root is $n-1$, and Equation~\ref{eq:flow} states that each of the other $n-1$ nodes consumes exactly one flow unit. Last, Equation~\ref{eq:tie} ties the auxiliary variables to the flow variables, making sure that flow occurs only on edges. The combination of these constraints guarantees that the graph induced by the variables $z_{ij}$ is a directed tree and consequently the graph induced by the objective variables $\by$ is connected.

\paragraph{Chain structure} 
A connected graph where the degree of all nodes is $\leq 2$ is a chain. Table~\ref{tab:degree} presents nodes' degree and demonstrates that indeed process graphs are close to being chains. The following constraint bounds nodes' degree by 2:

\begin{align}
\forall_j \sum_{i<j} y_{ij} + \sum_{j<k} y_{jk} \leq 2
\end{align}

Since graph structures are not always chains we add this as a soft constraint, that is, we penalize the objective for each node with degree $>2$. Thus, our modified objective function is $\sum_{ijr} \theta_{ijr} y_{ijr} + \sum_{k \in \sK} \alpha_k C_k$, where $\sK$ is the set of soft constraints, $\alpha_k$ is the penalty, and $C_k$ indicates whether a constraint is violated. We tune the parameters $\alpha_k$ on a development set, as explained in Section~\ref{subsec:setup}.

\paragraph{Relation triangles} 
A triangle is a 3-tuple of relations $(f(t_i,t_j),f(t_j,t_k),f(t_i,t_k))$. Clearly, some triangles are impossible while others are quite common. In order to look for triangles that could potentially improve process extraction we counted how many times each possible triangle occurs in both the training data and the output of our pairwise classifier, and focused on those for which the classifier and the gold standard disagreed. We are interested in triangles that never occur in the training data but are predicted by the classifier, and triangles that are frequent in the gold standard but do not appear in the classifier output. Figure~\ref{fig:triad} illustrates the triangles found and Equations~\ref{eq:sametransitivity}-\ref{eq:prev} provide the corresponding ILP formulation. Soft constraints were incorporated by defining a reward $\alpha_k$ for each triangle type and expanding the set $\sK$ accordingly\footnote{We experimented with a reward for certain triangles or a penalty for others and empirically found that using rewards results in better performance on the development set.}. 

\begin{enumerate}[itemsep=0pt,topsep=0pt] 
\item \textsc{Same} transitivity (Figure~\ref{fig:triad}a): Co-reference transitivity has been used in past work \cite{Finkel08} and we incorporate it as a soft constraint that encourages triangles that respect transitivity: 
\begin{align}
y_{ijSAME} + y_{jkSAME} + y_{ikSAME} \geq 3 \label{eq:sametransitivity}
\end{align}

\item \textsc{Cause}-\textsc{Cotemp} (Figure~\ref{fig:triad}b): If $t_i$ causes both $t_j$ and $t_k$, then often $t_j$ and $t_k$ are co-temporal. E.g, in ``\emph{genetic drift} has led to a \emph{loss} of genetic variation and an \emph{increase} in the frequency of harmful alleles", a single event causes two subsequent events that occur simultaneously. We formulate this as a soft constraint:
\begin{align}
y_{ijCAUSES} + y_{ikCAUSES} + y_{jkCOTEMP} \geq 3 \label{eq:causecotemp}
\end{align}

\item \textsc{Cotemp} transitivity (Figure~\ref{fig:triad}c):  If $t_i$ is co-temporal with $t_j$ and $t_j$ is co-temporal with $t_k$, then usually $t_i$ and $t_k$ are either co-temporal or denote the same event. We formulate this as a soft constraint:
\begin{multline}
y_{ijCOTEMP} + y_{jkCOTEMP}  +y_{ikCOTEMP} \\ + y_{ikSAME} \geq 3 \label{eq:cotemptransitivity}
\end{multline}

\item \textsc{Same} contradiction (Figure~\ref{fig:triad}d): if $t_i$ is the same event as  $t_j$, then their temporal ordering with respect to a third trigger $t_k$ may result in a contradiction, e.g., if $t_i$ is before $t_k$, but $t_j$ is after $t_k$. We define 5 temporal categories that generate $5 \choose 2$ possible contradictions, but for brevity present just one representative hard constraint. Note that this constraint depends on co-reference and temporal relations being predicted jointly.
\begin{align}
y_{ijPREV} + y_{jkPREV} + y_{ikSAME} \leq 2 \label{eq:samecontradiction}
\end{align}
\item \textsc{Prev} contradiction (Figure~\ref{fig:triad}e): As mentioned (Section~\ref{sec:model}), if $t_i$ is immediately before $t_j$, and $t_j$ is immediately before $t_k$, then it can not be that $t_i$ is immediately before $t_k$ (hard constraint).
\begin{align} 
y_{ijPREV} + y_{jkPREV} - y_{ikNONE} \leq 1 \label{eq:prev}
\end{align}
\end{enumerate}.

We used the Gurobi optimization package\footnote{\url{www.gurobi.com}} to find an exact solution for our ILP, which contains $O(n^2|\sR|)$ variables and $O(n^3)$ constraints. We have also developed an equivalent formulation amenable to dual decomposition \cite{Reichart12}, which is a faster approximation method, but practically found that solving the problem exactly with Gurobi is quite fast (average/median time per process: 0.294 sec/0.152 sec).



