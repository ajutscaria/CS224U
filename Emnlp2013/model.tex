\section{Joint Model for Process Extraction}

Given a paragraph $\bx$ and a trigger set $\sT$ we wish to extract all event-event relations $E$. Similar to Do et al. \shortcite{Do12} our model consists of a local pairwise classifier and global constraints. We first introduce a classifier that is based on features from previous work (Section~\ref{subsec:pairwise}). Next, we describe novel features specific for process extraction (Section~\ref{subsec:pairwise-novel}). Last, we incorporate global constraints into our model using ILP formulations (Section~\ref{subsec:global}).

\subsection{Local pairwise classifier} \label{subsec:pairwise}

The local pairwise classifier predicts relations between all event mention pairs. In order to model the direction of relations, we expand the set $\sR$ to include the reverse of four directed relations: \textsc{Prev}-\textsc{Next},  \textsc{Super}- \textsc{Sub}, \textsc{Causes}-\textsc{Caused}, \textsc{Enables}-\textsc{Enabled}. After adding \textsc{None} to indicate no relation, and including the undirected relations \textsc{Cotemp} and \textsc{Same}, $\sR$ contains 11 relations. The classifier is hence a function $f:\sT \times \sT \rightarrow \sR$. As an example, $f(t_i,t_j)=\mbox{\textsc{Prev}}$ iff $f(t_j,t_i)=\mbox{\textsc{Next}}$. Let $n$ be the number of triggers in a process, and $t_i$ be the $i$-th trigger in its description. Since $f(t_i,t_j)$ completely determines $f(t_j,t_i)$, it suffices to consider only pairs with $i<j$. Note that the process graph $\sP$ is undirected under the new definition of $\sR$.

Table~\ref{tab:features} describes features from previous work~\cite{Chambers08,Do12} extracted for a trigger pair $(t_i,t_j)$. Some features were omitted since they did not yield improvement in performance on a development set, or they require gold annotations provided in TimeBank, which we do not have. To reduce sparseness, we convert nominalizations into their verbal forms when computing word lemmas, using WordNet's \cite{Fellbaum1998} derivation links.

\begin{table}[t]
\setlength{\tabcolsep}{5pt}
{\scriptsize
\hfill{}
\begin{tabular}{|p{1.3cm}|p{6cm}|}
\hline
\textbf{Feature} &\textbf{Description}\\
\hline
 POS & \scriptsize{Pair of POS tags} \\
Lemma & \scriptsize{Pair of lemmas} \\
Prep$^*$ & \scriptsize{Preposition lexeme, if in a prepositional phrase} \\
Sent. count & \scriptsize{Quantized number of sentences between triggers} \\
Word count & \scriptsize{Quantized number of words between triggers} \\
LCA & \scriptsize{Least common ancestor on constituency tree, if exists} \\
Dominates$^*$ & \scriptsize{Whether one trigger dominates other} \\
Share & \scriptsize{Whether triggers share a child on dependency tree} \\
Adjacency & \scriptsize{Whether two triggers are adjacent} \\
Words btw. & \scriptsize{For adjacent triggers, content words between triggers} \\
%between & \\
Temp. btw. & \scriptsize{For adjacent triggers, temporal connectives (from a small list) between triggers} \\
\hline
\end{tabular}}
\hfill{}
\caption{Features extracted for a trigger pair $(t_i,t_j)$. Asteriks (*) indicate features that are duplicated, once for each trigger.}
\label{tab:features}
\end{table}

\subsection{Classifier extensions} \label{subsec:pairwise-novel}

A central source of information to extract event-event relations from text are \emph{connectives} such as \emph{after}, \emph{during}, etc. However, there is variability in the occurrence of these connectives as demonstrated by the following two sentences (connectives in bold, triggers in italics):

\begin{enumerate}[itemsep=0pt,topsep=0pt] \footnotesize{
\item \textbf{Because} alleles are \emph{exchanged} during \emph{gene flow}, genetic differences are \emph{reduced}. \label{sent:1}
\item During \emph{gene flow}, alleles are \emph{exchanged}, and genetic differences are \textbf{hence} \emph{reduced}. \label{sent:2}}
\end{enumerate}

Even though both sentences express the same relation $(\mbox{\emph{exchanged}},\mbox{\emph{reduced}},\mbox{\textsc{Causes}})$, the connectives used and their linear position with respect to the triggers are different. Also, in sentence~\ref{sent:1}, \emph{gene flow} intervenes between \emph{exchanged} and \emph{reduced}. Our dataset being small, we wish to identify the triggers related to each connective, and share features between such sentences. We do this using the syntactic structure and by clustering the connectives.

Sentence~\ref{sent:1} presents a typical case where by walking up the dependency tree from the marker \emph{because}, we can find the triggers related by this marker: $\mbox{\emph{because}}\xleftarrow{\scriptscriptstyle mark}\mbox{\emph{exchanged}}\xleftarrow{\scriptscriptstyle advcl}\mbox{\emph{reduced}}$. Whenever a trigger is the head of an adverbial clause and marked by a \emph{mark} dependency label, we walk on the dependency tree and look for a trigger in the main clause that is closest to the root (or the root itself in this example). 
By utilizing the syntactic structure, we can correctly spot that the trigger \emph{gene flow} is not related to the trigger \emph{exchanged} through the connective \emph{because}, even though they are linearly closer. In order to reduce sparseness of connectives, we created a hand-made clustering of 30 connectives that maps words into clusters (e.g. \emph{because}, \emph{since} and \emph{hence} to a ``causality" cluster). After locating the relevant pair of triggers, we use these clusters to fire the same feature for connectives belonging to the same cluster. We perform a similar procedure whenever a trigger is part of a prepositional phrase (imagine sentence~\ref{sent:1} starting with \emph{``due to allele exchange during gene flow \ldots"}) by walking up the constituency tree, but details are omitted for brevity. In sentence~\ref{sent:2}, the connective \emph{hence} is an adverbial modifier of the trigger \emph{reduced}. We look up the cluster for the connective \emph{hence} and fire the same feature for the adjacent triggers \emph{exchanged} and \emph{reduced}.

%One of the signals we use to identify connectives between event triggers is to use markers in dependency graph of the sentence. A marker of an adverbial clausal complement (advcl) is the word introducing it and carries information as to how triggers are related. We also use adverbial modifier relations that modifies the meaning of the word as another signal. In sentence 1 in the above example, \emph{because} acts as a marker between the triggers \emph{exchanged} and  \emph{reduced}. But, in sentence 2, \emph{hence} is an adverbial modifier to \emph{reduced}. We see that even though the sentences have the same semantics, the dependency relations ensued is very different. In fact, \emph{because} and \emph{hence} indicates that one trigger causes the other, but, $because X, Y$ translates to $X hence Y$. Hence, we would like the same set of features to be fired for the pair of triggers for both the sentences. To this end, we cluster the different marker and advmod relations based on the relation type it indicates. While appearing in the text between the event triggers, \emph{because} indicates $CAUSED$, whereas \emph{hence} indicates $\textsc{Causes}$. But, when \emph{because} appears before the first trigger, the causal relation is inverted, and as a result, it indicates $\textsc{Causes}$ as well. We extract a marker to indicate a relation between a pair of triggers if one is related to the other by an adverbial clause and the former has a marker attached to it. For advmod, we extract the relation if the two triggers appear consecutive in text and the latter has an advmod attached to it. Instead of using the actual marker or adverbial modifier, we insert the cluster name as the feature. We see that for both the sentences, the same cluster feature is fired. We also include connectives based on prepositional phrases, but will not go into details for brevity.

We further extend our features to handle the rich relation set necessary for process extraction. The first event of a process is often expressed as a nominalization and includes subsequent events (\textsc{Super} relation), e.g., ``The \emph{Calvin cycle} begins by \emph{incorporating}...". To capture this, we add a feature that fires when the first event of the process is a noun. We also add two features targeted at the \textsc{Same} relation: one indicating if the lemmas of $t_i$ and $t_j$ are same, and another specifying the determiner of $t_j$, if it exists. Certain determiners indicate that an event trigger has already been mentioned, e.g., the determiner \emph{this} hints a \textsc{Same} relation in ``The next steps \emph{decompose} citrate back to oxaloacetate. This \emph{regeneration} makes \ldots". Last, we add as a feature the dependency path between $t_i$ and $t_j$, if it exists, e.g., in ``meiosis produces cells that divide \ldots", the feature $\xrightarrow{\scriptscriptstyle dobj} \xrightarrow{\scriptscriptstyle rcmod}$ is fired for the trigger pair \emph{produces} and \emph{divide} . In Section~\ref{subsec:results} we empirically show that our extensions to the local classifier substantially improves performance.

For our pairwise classifier, we train a maximum entropy classifier that provides a probability $p_{ijr}$ for every trigger pair $(t_i,t_j)$ and relation $r$. Hence, $f(t_i,t_j)= \arg\max_r p_{ijr}$.

\subsection{Global Constraints} \label{subsec:global}
Naturally, pairwise classifiers are local models that can violate global properties in process structure. Figure~\ref{fig:graph} (left) presents an example for predictions made by the pairwise classifier, which result in two triggers (\emph{deleted} and \emph{depleted}) that are isolated from the rest of the triggers. In this section, we discuss how we incorporate constraints into our model to generate coherent global process structures.

Let $\theta_{ijr}$ be the score for a relation $r$ between the trigger pair $(t_i,t_j)$ (e.g. $\theta_{ijr}=\log p_{ijr}$), and $y_{ijr}$ be the corresponding indicator variable. Our goal is to find an assignment for the indicators $\by=\{y_{ijr} \ | \ 1 \leq i < j \leq n, r \in \sR \}$. With no global constraints this can be formulated as the following ILP:

\begin{align}
\vspace{-3mm}
\argmax_{\by} \sum_{ijr} \theta_{ijr} y_{ijr} \\
\mbox{s.t.} \forall_{i,j} \sum_r y_{ijr}=1 \nonumber
\vspace{-3mm}
\end{align}
\noindent where the constraint ensures exactly one relation between each event pair. We now describe constraints that results in a coherent global process structure:

\paragraph{Connectivity} 
Our ILP formulation for enforcing connectivity is a minor variation of the one suggested by Martins et al. \shortcite{Martins09} for dependency parsing. In our setup, we want $\sP$ to be a connected undirected graph, and not a directed tree. However, an undirected graph $\sP$ is connected iff there is a directed tree that is a subgraph of $\sP$ when edge directions are ignored. Thus the resulting formulation is almost identical and is based on flow constraints which ensure that there is a path from a designated root in the graph to all other nodes.

 Let $\bar{\sR}$ be the set $\sR \setminus \mbox{\textsc{None}}$. An edge $(t_i,t_j)$ is in $E$ iff there is some non-\textsc{None} relation between $t_i$ and $t_j$, i.e. iff $y_{ij}:=\sum_{r \in \bar{\sR}} y_{ijr}$ is equal to $1$. For each variable $y_{ij}$ we define two auxiliary binary variables $z_{ij}$ and $z_{ji}$ that correspond to edges of the directed tree that is a subgraph of $\sP$. We ensure that the edges in the tree exist also in $\sP$ by tying each auxiliary variable to its corresponding ILP variable:
\begin{align}
\vspace{-3mm}
\forall_{i<j} \ z_{ij} \leq y_{ij}, z_{ji} \leq y_{ij}
\vspace{-3mm}
\end{align}

Next, we add constraints that ensure the graph structure induced by the auxiliary variables is a tree rooted in an arbitrary node $1$ (The choice of root does not affect connectivity). We add for every $i \neq j$ a flow variable $\phi_{ij}$ which specifies the amount of flow on the directed edge $z_{ij}$.

\begin{align}
\vspace{-3mm}
\small &\sum_{i} z_{i1} =0, \forall_{j \neq 1} \sum_{i} z_{ij}=1 \label{eq:oneparent} \\ 
&\sum_{i} \phi_{1i}=n-1 \label{eq:rootflow} \\ 
&\forall_{j \neq 1} \ \sum_{i} \phi_{ij} - \sum_{k} \phi_{jk}=1 \label{eq:flow} \\
&\forall_{i \neq j} \ \phi_{ij} \leq n \cdot z_{ij} \label{eq:tie} 
\end{align}

Equation~\ref{eq:oneparent} says that all nodes in the graph have exactly one parent, except for the root that has no parents. Equation~\ref{eq:rootflow} ensures that the outgoing flow from the root is $n-1$, and Equation~\ref{eq:flow} states that each of the other $n-1$ nodes consume exactly one unit of flow. Last, Equation~\ref{eq:tie} ties the auxiliary variables to the flow variables, making sure that flow occurs only on edges. The combination of these constraints guarantees that the graph induced by the variables $z_{ij}$ is a directed tree and consequently the graph induced by the objective variables $\by$ is connected.

\paragraph{Chain structure} 
A chain is a connected graph where the degree of all nodes is $\leq 2$. Table~\ref{tab:degree} presents nodes' degree and demonstrates that indeed process graphs are close to being chains. The following constraint bounds nodes' degree by 2:

\begin{align}
\forall_j (\sum_{i<j} y_{ij} + \sum_{j<k} y_{jk} \leq 2)
\end{align}

Since graph structures are not always chains, we add this as a soft constraint, that is, we penalize the objective for each node with degree $>2$. The chain structure is one of the several soft constraints we enforce. Thus, our modified objective function is $\sum_{ijr} \theta_{ijr} y_{ijr} + \sum_{k \in \sK} \alpha_k C_k$, where $\sK$ is the set of soft constraints, $\alpha_k$ is the penalty (or reward desirable structures), and $C_k$ indicates whether a constraint is violated. We tune the parameters $\alpha_k$ on a development set, as explained in Section~\ref{subsec:setup}.

\paragraph{Relation triads} 
A relation triad (or a relation triangle) for any three triggers $t_{i}$, $t_{j}$ and $t_{k}$ in a process is a 3-tuple of relations $(f(t_i, t_j),f(t_j, t_k),f(t_i, t_k))$. Clearly, some triads are impossible while others are quite common. To find triads that could improve process extraction, the frequency of all possible triads in both the training set and the output of the pairwise classifier were found, and we focused on those for which the classifier and the gold standard disagreed. We are interested in triads that never occur in training data but are predicted by the classifier, and vice versa. Figure~\ref{fig:triad} illustrates some of the triads found and Equations~\ref{eq:sametransitivity}-\ref{eq:prev} provide the corresponding ILP formulations. Equations~\ref{eq:sametransitivity}-\ref{eq:cotemptransitivity} were formulated as soft constraints (expanding the set $\sK$) and were incorporated by defining a reward $\alpha_k$ for each triad type \footnote{We experimented with a reward for certain triads or a penalty for others and empirically found that using rewards results in better performance on the development set.}. On the other hand, Equations~\ref{eq:samecontradiction}-\ref{eq:prev} were formulated as hard constraints to prevent certain structures.

\begin{enumerate}[itemsep=0pt,topsep=0pt] 
\item \textsc{Same} transitivity (Figure~\ref{fig:triad}a, Eqn.~\ref{eq:sametransitivity}): Co-reference transitivity has been used in past work \cite{Finkel08} and we incorporate it by a constraint that encourages triads that respect transitivity.
\item \textsc{Cause}-\textsc{Cotemp} (Figure~\ref{fig:triad}b, Eqn.~\ref{eq:causecotemp}): If $t_i$ causes both $t_j$ and $t_k$, then often $t_j$ and $t_k$ are co-temporal. E.g, in ``\emph{genetic drift} has led to a \emph{loss} of genetic variation and an \emph{increase} in the frequency of $\ldots$", a single event causes two subsequent events that occur simultaneously. 
\item \textsc{Cotemp} transitivity (Figure~\ref{fig:triad}c, Eqn.~\ref{eq:cotemptransitivity}):  If $t_i$ is co-temporal with $t_j$ and $t_j$ is co-temporal with $t_k$, then usually $t_i$ and $t_k$ are either co-temporal or denote the same event. 
\item \textsc{Same} contradiction (Figure~\ref{fig:triad}d, Eqn.~\ref{eq:samecontradiction}): if $t_i$ is the same event as  $t_k$, then their temporal ordering with respect to a third trigger $t_j$ may result in a contradiction, e.g., if $t_j$ is after $t_i$, but before $t_k$. We define 5 temporal categories that generate $5 \choose 2$ possible contradictions, but for brevity present just one representative hard constraint. This constraint depends both on co-reference and temporal relations predicted.
\item \textsc{Prev} contradiction (Figure~\ref{fig:triad}e, Eqn.~\ref{eq:prev}): As mentioned (Section~\ref{sec:model}), if $t_i$ is immediately before $t_j$, and $t_j$ is immediately before $t_k$, then $t_i$ is not immediately before $t_k$.
\end{enumerate}.
\vspace{-4mm}
\begin{flalign}
 y_{ij\mathsf{\textsc{Same}}}     + y_{jk\mathsf{\textsc{Same}}} + y_{ik\mathsf{\textsc{Same}}} \geq 3  &  \label{eq:sametransitivity} \\
  y_{ij\mathsf{\textsc{Causes}}}  +  y_{ik\mathsf{\textsc{Causes}}} + y_{jk\mathsf{\textsc{Cotemp}}}  \geq 3 &  \label{eq:causecotemp} \\
  y_{ij\mathsf{\textsc{Cotemp}}} +  y_{jk\mathsf{\textsc{Cotemp}}}  +  y_{ik\mathsf{\textsc{Cotemp}}} +  & \nonumber  \\ 
   y_{ik\mathsf{\textsc{Same}}}  \geq 3  & \label{eq:cotemptransitivity} \\
  y_{ij\mathsf{\textsc{Prev}}} + y_{jk\mathsf{\textsc{Prev}}} + y_{ik\mathsf{\textsc{Same}}} \leq 2 & \label{eq:samecontradiction} \\
  y_{ij\mathsf{\textsc{Prev}}} + y_{jk\mathsf{\textsc{Prev}}} - y_{ik\mathsf{\textsc{None}}} \leq 1& \label{eq:prev}
\end{flalign}

We used the Gurobi optimization package\footnote{\url{www.gurobi.com}} to find an exact solution for our ILP, which contains $O(n^2|\sR|)$ variables and $O(n^3)$ constraints. We also developed an equivalent formulation amenable to dual decomposition \cite{dualdecomp}, which is a faster approximation method. But practically, solving the ILP exactly with Gurobi is quite fast (average/median time per process: 0.294 sec/0.152 sec).



