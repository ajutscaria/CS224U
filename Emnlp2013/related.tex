\section{Related Work}
As we mentioned in the introduction, a closely related line of work is biomedical event extraction in recent BioNLP shard tasks \cite{kim09,kim11}. 
%To capture the rich relations and complex nested event types annotated in BioNLP dataset, \newcite{Miwa10} proposed a classification approach with a rich set of features specifically designed for complex relations. 
Traditional approach to event extraction employs a pipeline architecture where events are identified first, typically done using classifiers with rich set of features \cite{Miwa10}, arguments of the candidate events are then identified next \cite{Bjorne11}. 
\newcite{Poon10} showed improved results using Markov logic to jointly predict events and arguments. 
\newcite{Mcclosky11} observed that the arguments in nested events exhibit a tree-like structure. They proposed an approach to extract such structure using dependency parsing algorithms.

One limitation in the setting of these earlier work in event extraction is that events that occur together are considered independently, and the context is limited to a single sentence. \newcite{riedel11fast} presented three successive models that capture the correlations between events, and enforces consistency across arguments. They showed an efficient joint-inference algorithm using dual decomposition techniques. 
Temporal event-to-event relations have been extensively studied before \cite{Chambers07,Chambers08,DSouzaNg:13a}. We leverage the  techniques used in temporal relation extraction in our work and borrow some features from \cite{Chambers07}. But one importance is that we extend beyond the simple types of temporal relations (e.g., \textit{before},\textit{after} and \textit{overlap}) to a richer set that includes \textit{cause}, \textit{enable} and \textit{super-event}.

 \newcite{Chambers08ACL} learns narrative events and arguments using distributional similarities, and then resort to a temporal classifier to link the events in temporal order in a chain structure. In our work, we make no such assumption of a chain structure, and predict more complex structures. \newcite{Cheung13} also learns to construct a event template (a.k.a. \textit{frame}) from text using unsupervised generative models. A major difference in our work is that we do not have the abundance of data as in frame learning setting, where common events and their arguments are observed many times; instead we are given one paragraph and our model has a ``one-shot'' chance at extracting the process structure.
 
We showed in this paper that global structural properties can lead to significant improvements in process extraction accuracy, and ILP is an effective framework for modeling global constraints. Similar observations and techniques have also been proposed in other information extraction tasks. 
\newcite{Reichart12} ties information from multiple sequence models that describe the same event by using global higher-order potentials. 
\newcite{CLBerant} proposed a global inference algorithm to identify entailment relations. 
\newcite{Do12} models a set of global temporal order constraints also using ILP for timeline construction. 
There is abundance of examples of enforcing global constraints using approximate in other NLP tasks, such as in coreference resolution \cite{Finkel08}, parsing \cite{Rush12} and named entity recognition \cite{Wang13}.
