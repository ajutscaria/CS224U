\section{Related Work}
As  previously mentioned, a related line of work is biomedical event extraction in recent BioNLP shared tasks \cite{kim09,kim11}. 
Earlier work employed a pipeline architecture where first events are found, and then their arguments are identified \cite{Miwa10,Bjorne11}. Subsequent methods proposed to predict events and arguments jointly using Markov logic \cite{Poon10} and dependency parsing algorithms \cite{Mcclosky11}. \newcite{riedel11fast} further improved performance by by capturing correlations between events and enforcing consistency across arguments.

%To capture the rich relations and complex nested event types annotated in BioNLP dataset, \newcite{Miwa10} proposed a classification approach with a rich set of features specifically designed for complex relations. 
%Traditional event extraction employs a pipeline architecture where events are identified first, typically done using classifiers with rich set of features \cite{Miwa10}, arguments of the candidate events are then identified next \cite{Bjorne11}. 
%\newcite{Poon10} showed improved results using Markov logic to jointly predict events and arguments. 
%\newcite{Mcclosky11} observed that the arguments in nested events exhibit a tree-like structure. They proposed an approach to extract such structure using dependency parsing algorithms.

%One limitation in the setting of these earlier work in event extraction is that events that occur together are considered independently, and the context is limited to a single sentence. \newcite{riedel11fast} presented three successive models that capture the correlations between events, and enforces consistency across arguments. They showed an efficient joint-inference algorithm using dual decomposition techniques. 

Temporal event-event relations have been extensively studied \cite{Chambers08,Yoshikawa09,Denis11,Do12,Mcclosky12,DSouzaNg:13a}, and we leverage such techniques in our work (Section~\ref{subsec:pairwise}). However, we extend beyond temporal relations alone, and strongly rely on dependencies between process events. \newcite{Chambers11} learned event templates (or frames), where events that are related to one another and their semantic roles are extracted. Recently, \newcite{Cheung13} proposed an unsupervised generative model for inducing such templates. A major difference in our work is that we do not learn typical event relations from a large and redundant corpus, but are given a paragraph and have a ``one-shot'' chance to extract the process structure.

%\newcite{Chambers08ACL} learned narrative events and arguments using distributional similarities, and then resort to a temporal classifier to link the events in temporal order in a chain structure. In our work, we make no such assumption of a chain structure, and predict more complex structures. \newcite{Cheung13} also learns to construct a event template (a.k.a. \textit{frame}) from text using unsupervised generative models. A major difference in our work is that we do not have the abundance of data as in frame learning setting, where common events and their arguments are observed many times; instead we are given one paragraph and our model has a ``one-shot'' chance at extracting the process structure.

%is that we extend beyond the simple types of temporal relations (e.g., \textit{before},\textit{after} and \textit{overlap}) to a richer set that includes \textit{cause}, \textit{enable} and \textit{super-event}. 
 
We showed in this paper that global structural properties lead to significant improvements in extraction accuracy, and ILP is an effective framework for modeling global constraints. Similar observations and techniques have been proposed in other information extraction tasks. 
\newcite{Reichart12} ties information from multiple sequence models that describe the same event by using global higher-order potentials. 
\newcite{CLBerant} proposed a global inference algorithm to identify entailment relations. 
\newcite{Do12} model a set of global temporal order constraints also using ILP for timeline construction. 
There is abundance of examples of enforcing global constraints using approximate methods in other NLP tasks, such as in coreference resolution \cite{Finkel08}, parsing \cite{Rush12} and named entity recognition \cite{Wang13}.
